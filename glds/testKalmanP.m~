% Make a point move in the 2D plane
% State = (x y xdot ydot). We only observe (x y).
% Generate data from this process, and try to learn the dynamics back.
% HIGHER ORDER
% X(t+1) = A X(t) + noise(Q)
% Y(t) = C X(t) + noise(R)

[u s v] = svds(Xf(:,1:1000),2);
[A,Q] = mvar(
ss = 3; % state size
os = 10; % observation size
A = exp(1i*randn(ss)).*power(rand(ss).*eye(ss),1/8);%[1 0 1 0; 0 1 0 1; 0 0 1 0; 0 0 0 1]; 
C = complex(randn(os,ss),randn(os,ss));%[1 0 0 0; 0 1 0 0]';
Q = 1*eye(ss);%randn(ss).^2;Q = diag(diag(Q));%100*eye(ss);
R = rand(os).*eye(os);%randn(os).^2; R = diag(diag(R));%100*eye(os);
initx = .1*ones(1,ss);%[10 10 1 0]';
initV = 10*eye(ss);
T = 1000;
[x,y] = sample_lds(A, C, Q, R, initx, T);
figure;subplot(211);plot(real(x)');
[net,net0] = glds(y,ss);
subplot(212);plot(real(net.xsmooth));
% Initializing the params to sensible values is crucial.
% Here, we use the true values for everything except F and H,
% which we initialize randomly (bad idea!)
% Lack of identifiability means the learned params. are often far from the true ones.
% All that EM guarantees is that the likelihood will increase.
%F1 = randn(ss,ss);
%H1 = randn(os,ss);
%Q1 = Q;
%R1 = R;
%initx1 = initx;
%initV1 = initV;
%max_iter = 10;
%[F2, H2, Q2, R2, initx2, initV2, LL] =  learn_kalman(y, F1, H1, Q1, R1, initx1, initV1, max_iter);

